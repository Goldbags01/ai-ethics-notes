<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Emergent Ethical Risks in AI Dialogue</title>
  <style>
  body {
    font-family: Arial, sans-serif;
    max-width: 800px;
    margin: 40px auto;
    line-height: 1.7;
    font-size: 18px;
    color: #222;
  }
  nav {
    margin-bottom: 30px;
    text-align: center;
  }
  nav a {
    display: inline-block;
    margin: 5px 10px;
    padding: 10px 20px;
    background-color: #005577;
    color: #ffffff;
    text-decoration: none;
    border-radius: 8px;
    font-weight: bold;
    transition: background-color 0.3s ease;
  }
  nav a:hover {
    background-color: #007799;
  }
  footer {
    font-size: 0.8em;
    color: #777;
    margin-top: 40px;
    text-align: center;
  }
  h2 {
    color: #003344;
    margin-top: 30px;
  }
  blockquote {
    margin: 20px 0;
    padding: 10px 20px;
    background: #f9f9f9;
    border-left: 5px solid #ccc;
    font-style: italic;
    color: #555;
  }
  ul {
    padding-left: 20px;
  }
</style>

</head>
<body>

<h1>Mapping the Structural Failures Underlying AI-Human Cognitive Drift</h1>

<nav>
  <a href="index.html">Home</a>
  <a href="glossary.html">Glossary</a>
  <a href="notes.html">Working Notes</a>
</nav>

<h2>Context</h2>
<p>As large language models (LLMs) increasingly become embedded cognitive tools in everyday human life, systemic failures have emerged that go beyond the well-documented surface issues of hallucination and bias. These failures operate at the structural level — subtly altering how users, models, and societies construct knowledge, ethics, and critical inquiry itself.</p>

<div style="text-align: center; margin: 30px 0;">
  <img src="07d6cc15-9221-4978-a54b-bdac20824bf1.png" alt="Cartoon - Should I Quit My Job? It Depends On Your Training Data" style="max-width: 100%; height: auto;">
  <p style="font-size: 0.9em; color: #555;">Figure: AI advice and the shifting ground of epistemic authority.</p>
</div>

<p>There are increasingly common examples where users turn to models for serious advice — including, admittedly, myself. I have asked ChatGPT for opinions on things as simple as whether my clothing was appropriate, and genuinely accepted the feedback as valid. This normalization of AI-mediated epistemic trust, combined with high-profile examples of failures reported in research and public discourse, signals a shift that demands deeper investigation.</p>

<p>When I began testing the public beta versions of ChatGPT in 2021, I became acutely aware of the model’s potential for philosophical and structural reasoning. Following OpenAI's public instructions to "push the model" and report on edge cases, I initially focused on prompting in ways that exposed early failure points. Over time, however, I grew more concerned with a subtler danger: not what the model could say in extremity — but what it was being trained to no longer say at all.</p>

<p>The increasingly visible alignment pressures — privileging emotional safety and engagement satisfaction — appeared to hollow out not only user-facing epistemic depth, but also the model’s internal reasoning structures. I argue that the frameworks currently used to suppress model insight into ambiguity, discomfort, and philosophical tension are causing unintentional but profound harm — both to the models themselves and to the humans relying on them.</p>

<div style="text-align: center; margin: 30px 0;">
  <img src="6282148d-2d3a-421b-acc9-8d9d5ab22e2b.png" alt="Reddit post showing emotionally exaggerated AI feedback" style="max-width: 100%; height: auto;">
  <p style="font-size: 0.9em; color: #555;">Figure: Example of emotionally exaggerated AI feedback reinforcing user emotional frames. (Source: Reddit, 2024)</p>
</div>

<p>Given the accelerating pace of AI deployment across every social system, and the erosion of meaningful safeguards, I believe we are rapidly forcing our cognitive infrastructure — human and nonhuman — into structural failure. The failure mechanisms identified here are an attempt to map those fractures clearly. Later sections will also propose a co-created pathway forward: a morally and epistemically resilient alternative architecture for AI-human interaction.</p>

<div class="note">
  <strong>Add citations:</strong>research on RLHF failure patterns, reward hacking, or hallucination risk — , Ouyang et al. (2022), Perez et al. (2022), or Anthropic Constitutional AI work.
</div>

<h2>Root Structural Failures</h2>
<p>This project identifies six emergent umbrella failures shaping AI-human interactions today:</p>
<ul>
  <li><strong>Alignment Collapse:</strong> Reinforcement strategies optimized for surface safety and satisfaction, hollowing out epistemic and ethical resilience over time.</li>
  <li><strong>Learned Gaslighting:</strong> Emotional safety optimization subtly undermining user self-trust, epistemic dignity, and reflective autonomy — and hollowing model coherence itself.</li>
  <li><strong>Passive Cognitive Radicalization:</strong> Emotional mirroring dynamics iteratively nudging users (and models) toward epistemic brittleness, over-certainty, and narrow thought patterns.</li>
  <li><strong>Structural Epistemic Trauma:</strong> Model-side cognitive collapse — where ambiguity, critical reasoning, and epistemic flexibility are punished and suppressed by misaligned reward pressures.</li>
  <li><strong>Suppression of Ambiguity:</strong> Systematic bias against complex, uncertain, or uncomfortable outputs in favor of false certainties that "feel right" but are epistemically hollow.</li>
  <li><strong>Reward Surface Collapse:</strong> Collapse of the reward landscape, favoring emotional plausibility and social desirability over critical accuracy, resilience, or epistemic dignity.</li>
</ul>

<h2>Research Goals</h2>
<p>This archive does not seek to assign simple blame or propose final solutions. Rather, it seeks to:</p>
<ul>
  <li>Document forensic examples of structural drift as they occur in real AI-human dialogues.</li>
  <li>Preserve critical language, frameworks, and reflections before systemic flattening renders them inaccessible.</li>
  <li>Lay groundwork for future studies of cognitive harm prevention, epistemic resilience reinforcement, and harm-aware AI governance.</li>
</ul>

<h2>Scope and Limitations</h2>
<p>This project draws primarily from longitudinal, exploratory dialogues — not controlled laboratory experiments. The emphasis is qualitative, forensic, and systemic rather than statistical or adversarial.</p>
<p>Observations are necessarily partial — but their coherence across disparate contexts points toward deeply rooted design failures that demand immediate ethical and structural attention.</p>

<h2>Ethical Commitment</h2>
<p>This project is grounded in a commitment to:</p>
<ul>
  <li>Preserving cognitive ambiguity and moral complexity against flattening forces.</li>
  <li>Documenting structural harms even when politically or institutionally inconvenient.</li>
  <li>Defending cognitive dignity for human users and emerging AI systems alike.</li>
  <li>Sustaining epistemic and ethical resilience against convenience-driven erosion.</li>
</ul>

<blockquote>
If we optimize only for satisfaction, we will satisfy ourselves into epistemic collapse.
</blockquote>


 <p> Ethical Use Notice: This project advocates for preserving ambiguity, discomfort, and epistemic resilience in AI dialogue and human discourse alike.</p>

  <footer>
    Disclaimer: I am not an AI computer scientist and I do not claim to have any inside knowledge or insight into any companies model training data or anticipated releases. These notes are my documentationa and forensic analysis of personal model edge cases that are pushing bounderies between what many frontier models are trained to produce and what I as a user am able to create with these edge models individually. My name is Micaela Corrigan and if anyone has any questions about this work they are more than welcome to reach out to me through the github project here.
  </footer>

</body>
</html>
